Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	dge_deseq2
	4	download_reads
	1	download_transcriptome
	1	make_salmon_index
	4	salmon_count_se
	12

[Sat Dec  5 00:26:14 2020]
rule download_reads:
    output: raw_data/SRR1761159.fq.gz
    jobid: 3
    wildcards: sample=SRR1761159

Activating conda environment: /Users/biplab/Documents/rna_seq_workflow/.snakemake/conda/c5b7b2c8
Terminating processes on user request, this might take some time.
[Sat Dec  5 00:26:24 2020]
Error in rule download_reads:
    jobid: 3
    output: raw_data/SRR1761159.fq.gz
    conda-env: /Users/biplab/Documents/rna_seq_workflow/.snakemake/conda/c5b7b2c8
    shell:
        fastq-dump -Z --gzip SRR1761159 > raw_data/SRR1761159.fq.gz
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job download_reads since they might be corrupted:
raw_data/SRR1761159.fq.gz
Complete log: /Users/biplab/Documents/rna_seq_workflow/.snakemake/log/2020-12-05T002613.298274.snakemake.log
