Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	dge_deseq2
	1	download_reads
	1	make_salmon_index
	4	salmon_count_se
	8

[Fri Dec  4 14:37:49 2020]
rule download_reads:
    output: raw_data/SRR1761158.fq.gz
    jobid: 1
    wildcards: sample=SRR1761158

Activating conda environment: /Users/biplab/Documents/rna_seq_workflow/.snakemake/conda/c5b7b2c8
[Fri Dec  4 14:48:00 2020]
Error in rule download_reads:
    jobid: 1
    output: raw_data/SRR1761158.fq.gz
    conda-env: /Users/biplab/Documents/rna_seq_workflow/.snakemake/conda/c5b7b2c8
    shell:
        fastq-dump -Z --gzip SRR1761158 > raw_data/SRR1761158.fq.gz
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job download_reads since they might be corrupted:
raw_data/SRR1761158.fq.gz
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/biplab/Documents/rna_seq_workflow/.snakemake/log/2020-12-04T143748.264823.snakemake.log
